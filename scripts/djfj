import pickle
import os
from google_auth_oauthlib.flow import Flow, InstalledAppFlow
from googleapiclient.discovery import build
from googleapiclient.http import MediaFileUpload, MediaIoBaseDownload
from google.auth.transport.requests import Request


def Create_Service(client_secret_file, api_name, api_version, *scopes):
    print(client_secret_file, api_name, api_version, scopes, sep='-')
    CLIENT_SECRET_FILE = client_secret_file
    API_SERVICE_NAME = api_name
    API_VERSION = api_version
    SCOPES = [scope for scope in scopes[0]]
    print(SCOPES)

    cred = None

    pickle_file = f'token_{API_SERVICE_NAME}_{API_VERSION}.pickle'
    # print(pickle_file)

    if os.path.exists(pickle_file):
        with open(pickle_file, 'rb') as token:
            cred = pickle.load(token)

    if not cred or not cred.valid:
        if cred and cred.expired and cred.refresh_token:
            cred.refresh(Request())
        else:
            flow = InstalledAppFlow.from_client_secrets_file(CLIENT_SECRET_FILE, SCOPES)
            cred = flow.run_local_server()

        with open(pickle_file, 'wb') as token:
            pickle.dump(cred, token)

    try:
        service = build(API_SERVICE_NAME, API_VERSION, credentials=cred)
        print(API_SERVICE_NAME, 'service created successfully')
        return service
    except Exception as e:
        print('Unable to connect.')
        print(e)
        return None

def convert_to_RFC_datetime(year=1900, month=1, day=1, hour=0, minute=0):
    dt = datetime.datetime(year, month, day, hour, minute, 0).isoformat() + 'Z'
    return dt
#!/usr/bin/env python
# coding: utf-8

# In[1]:


# import os;
from pathlib import Path;
from scripts.Google import Create_Service;
from scripts.readSheets import *;
import pandas as pd;
import numpy as np;
import re;




FOLDER_PATH = Path.cwd()
FILE_NAME = 'client_secret_file.json'
API_NAME = 'sheets'
API_VERSION = 'v4'
SCOPES = ['https://www.googleapis.com/auth/spreadsheets']



# In[4]:



CLIENT_SECRET_FILE = FOLDER_PATH / FILE_NAME
# CLIENT_SECRET_FILE = os.path.join(FOLDER_PATH, 'Client_Secret.json')

service = Create_Service(CLIENT_SECRET_FILE, API_NAME, API_VERSION, SCOPES)

def SheetsNew():
    """
    To specify Google Sheets file basic settings and as well as configure default worksheets
    """
    sheet_body = {
        'properties': {
            'title': 'ApiSheetsNew',
            'locale': 'en_US', # optional
            'timeZone': 'America/Los_Angeles'
            }
        ,
        'sheets': [
            {
                'properties': {
                    'title': 'Data'
                }
            },
            {
                'properties': {
                    'title': 'Pivot'
                }
            }
        ]
    }

    sheets_file2 = service.spreadsheets().create(body=sheet_body).execute()
    return {'Url':sheets_file2['spreadsheetUrl'],'gsheetId':sheets_file2['spreadsheetId'],'sheet_names':sheets_file2['sheets']}



# In[11]:

def createNewSpreadsheet():   
    sheet_body = {
        'properties': {
            'title': 'ApiSheetsNew',
            'locale': 'en_US', # optional
            'timeZone': 'America/Los_Angeles'
            }
        ,
        'sheets': [
            {
                'properties': {
                    'title': 'default'
                }
            }
        ]
    }

    sheets_file2 = service.spreadsheets().create(body=sheet_body).execute()
    return {'Url':sheets_file2['spreadsheetUrl'],'gsheetId':sheets_file2['spreadsheetId'],'sheet_names':sheets_file2['sheets']}


def copySheets():
    url='https://docs.google.com/spreadsheets/d/1z0U-WEYjoc8ByMUECr_zNSxTMPAVgyg3_w9pPuoVWDE/edit#gid=1891707339'
    sourcegSheetId=getSheetId(url)
    df = getSheetProperties(url)
    worksheet_id=list(map(lambda x: df[x]['properties']['sheetId'],range(len(df))))
    worksheet_names=list(map(lambda x: df[x]['properties']['title'],range(len(df))))
    df1 = createNewSpreadsheet()
    destgSheetId=getSheetId(df1['Url'])

    for i in worksheet_id:
        print('copying ' + str(i))
       	service.spreadsheets().sheets().copyTo(
        spreadsheetId=sourcegSheetId,
        sheetId=i,
        body={'destinationSpreadsheetId':destgSheetId}
        ).execute()
        copy_url = df1['Url']
        prop = getSheetProperties(copy_url)
        names=list(map(lambda x: prop[x]['properties']['title'],range(len(prop))))
        sheet_ids=list(map(lambda x: prop[x]['properties']['sheetId'],range(len(prop))))
        names=list(map(lambda x: x.replace('Copy of ',''),names))
        for indx, sheet_id in enumerate(sheet_ids):
            service.spreadsheets().batchUpdate(
            spreadsheetId=destgSheetId,
            body=request_template(sheet_id,names[indx])
            ).execute()
    return df1['Url']

def file_time_stamp(pattern):

    """
    Createing nest file
    """

    # read export
    # first define path
    location = 'Downloads'
    file_path = Path.cwd() / location

    # next assign path and define pattern of file name
    downloaded_file = Path.cwd() / 'Downloads'
    pattern = '*'+pattern+'*'

    # use funciton to get latest time of file
    return {'time': getLatestFileNameTime(downloaded_file, pattern) , 'file':getLatestFileName(downloaded_file,pattern)}

def writeDataToSheetDf(worksheet_name,gsheetId,df):
    cell_range_insert = 'A1'
    
    # Replace Null values
    df.replace(np.nan,'',inplace=True)

    # Convert all date columns to string type
    for col in  df.select_dtypes(include=['datetime64']).columns.tolist():
        df[col] = df[col].astype(str)

    response_date = service.spreadsheets().values().append(
        spreadsheetId=gsheetId,
        valueInputOption='RAW',
        range=worksheet_name,
        body=dict(
            majorDimension = 'ROWS',
            values = df.T.reset_index().T.values.tolist()
        )
    ).execute()



def writeDataToSheet(worksheet_name,gsheetId,file_name,sheet_name):
    cell_range_insert = 'A1'
    # read file in path
    file_string = str(file_name)
    if 'xls' not in file_string:
    	df = pd.read_csv(file_name)
    else:
    	df = pd.read_excel(file_name,sheet_name,index_col=False)

    # Replace Null values
    df.replace(np.nan,'',inplace=True)

    # Convert all date columns to string type
    for col in  df.select_dtypes(include=['datetime64']).columns.tolist():
        df[col] = df[col].astype(str)

    response_date = service.spreadsheets().values().append(
        spreadsheetId=gsheetId,
        valueInputOption='RAW',
        range=worksheet_name,
        body=dict(
            majorDimension = 'ROWS',
            values = df.T.reset_index().T.values.tolist()
        )
    ).execute()



# In[8]:


# method to add inv for nest pivot
def addInvValues(df):
    # read the fabric inventory
    df_inv = main('1tOkainSd6Q_cdwyPMpAyvs3ze2fCU20yhM1yp4VEHRc','Fabrics Master Data!A:E')
    options = ['Tony Stock','Justin Stock']
    for i in range(0,len(options)):
        df_loc = df_inv.loc[df_inv['Stock']==options[i]]
         # merge inventory to data table
        df = pd.merge(df,df_loc[['Fabric #','Fabric Yards']],on='Fabric #', how='left')
        temp = options[i] + ' Fabric Yards'
        df = df.rename(columns={"Fabric Yards": temp})
        df[temp] = df[temp].astype(str).astype(float)
      

    # get inv per wo line for pivot view cal field
    

    # so_count = []
    # list1 = nest_file['Sale Order Line/Qty to Produce'].tolist()
    # tempList = nest_file['Fabric Yards'].tolist()
    # for x in tempList:
    #     so_count.append(tempList.count(x))
    # ar1 = np.array(list1)
    # ar2 = np.array(so_count)
    # ar3 = ar1 / ar2
    # nest_file['Fabric Inv Per WO line'] = ar3
    return df


# method to create new sheets  tabs

def add_sheets(gsheetId, sheet_name):
    spreadsheets = service.spreadsheets()
    try:
        request_body = {
            'requests': [{
                'addSheet': {
                    'properties': {
                        'title': sheet_name,
                        'tabColor': {
                            'red': 0.44,
                            'green': 0.99,
                            'blue': 0.50
                        }
                    }
                }
            }]
        }

        response = spreadsheets.batchUpdate(
            spreadsheetId=gsheetId,
            body=request_body
        ).execute()

        return response
    except Exception as e:
        print(e)

def getSheetProperties(Url):
   
    gsheetId = re.sub(r'(?i)(^.+?/d/|/edit.*$)','',Url)
    print('this is the gsheetId ' + gsheetId)
    sheet_metadata = service.spreadsheets().get(spreadsheetId=gsheetId).execute()
    return sheet_metadata.get('sheets', '')

def getSheetId(Url):
    return re.sub(r'(?i)(^.+?/d/|/edit.*$)','',Url)


def clearSheets(gsheetId,worksheet_name):
    service.spreadsheets().values().clear(
        spreadsheetId = gsheetId,
        range=worksheet_name
    ).execute()

def clearSheetsRange(gsheetId,worksheet_name,cell_range):    
    service.spreadsheets().values().clear(
        spreadsheetId = gsheetId,
        range=worksheet_name + '/s' + cell_range
    ).execute()

def getLatestFileName (path: Path, pattern: str = "*"):
    files = path.rglob(pattern)
    return max(files,key=lambda x: x.stat().st_ctime)
def getLatestFileNameTime (path: Path, pattern: str = "*"):
    files = path.rglob(pattern)
    return max(files,key=lambda x: x.stat().st_ctime).stat().st_ctime

def request_template(sheet_id, sheet_name):
    request_body={
        'requests': [
            {
                'updateSheetProperties': {
                    'properties': {
                        'sheetId': sheet_id,
                        'title': sheet_name
                    },
                    'fields': 'title'
                }
            }
        ]
    }
    return request_body


def copyDiligentTemplate():
    url = copySheets()
    prop = getSheetProperties(url)
    names=list(map(lambda x: prop[x]['properties']['title'],range(len(prop))))
    sheet_ids=list(map(lambda x: prop[x]['properties']['sheetId'],range(len(prop))))
    names=list(map(lambda x: x.replace('Copy of ',''),names))
    gsheetId = getSheetId(url)
    for indx, sheet_id in enumerate(sheet_ids):
        service.spreadsheets().batchUpdate(
        spreadsheetId=gsheetId,
        body=request_template(sheet_id,names[indx])
        ).execute()
    return url

def createNestFile(pattern):
    """
    Creating nest file
    """

    # read export
    # first define path
    location = 'Downloads'
    file_path = Path.cwd() / location

    # next assign path and define pattern of file name
    downloaded_file = Path.cwd() / 'Downloads'
    pattern = '*'+pattern+'*'

    # use funciton to get latest file
    myFile = getLatestFileName(downloaded_file,pattern)

    # next read the contents of the file must be either csv or excel file
    if 'xls' not in str(myFile):
        nest_file = pd.read_csv(myFile,index_col=False)
    else:
        nest_file = pd.read_excel(myFile,sheet_name=0,index_col=False)
    return nest_file



def createCoversData(nest_file):
    # Mirror excel countif to generate sliced so line qty to produce
    tempList = nest_file['Sale Order Line/Product/Display Name'].fillna('blanks').tolist()
    bondi_chicory = []
    covers_rename = []
    for x in range(0,len(tempList)):
        if (re.search('(?i)(?:mst1 bondi|mst1 chicory)',tempList[x])!=None):
            bondi_chicory.append((re.search('(?i)(?:mst1 bondi|mst1 chicory)',tempList[x])).group(0))
        else:
            bondi_chicory.append('other')
    nest_file['BONDI|CHICORY'] = bondi_chicory

    covers_rename = nest_file['Product/Display Name'].fillna('blanks').tolist()
    for x in range(0,len(covers_rename)):
        covers_rename[x] = re.sub(r'(?i)(?:\[.+\]\s|\sv2$)','',covers_rename[x])

    nest_file['Covers'] = covers_rename
    rename = nest_file['Sale Order Line/Product/Display Name'].fillna('blanks').tolist()
    rename2 = nest_file['Product/Display Name'].fillna('blanks').tolist()
    for x in range(0,len(rename)):
        rename[x] = re.sub(r'(?i)(.*\]\s)','',rename[x])
        rename2[x] = re.sub(r'(?i)(.*\]\s)','',rename2[x])
    to_assign = {'Product Name':rename,'Component Name':rename2}
    nest_file = nest_file.assign(**to_assign)

    # use numpy to create countif of so quantity
    so_count = []
    list1 = nest_file['Sale Order Line/Qty to Produce'].tolist()
    tempList = nest_file['Sale Order Line/ID'].tolist()
    for x in tempList:
        so_count.append(tempList.count(x))
    ar1 = np.array(list1)
    ar2 = np.array(so_count)
    ar3 = ar1 / ar2
    nest_file['SO Line Product Qty'] = ar3
    ar4 =  nest_file['Quantity To Be Produced'].tolist()
    nest_file['Qty per SO qty'] = ar4 / ar1

    """
    sewing wo data detailed table

    """
    # creating column for fabric number

    # read the fabric color and number for mapping
    fabric_color = main('1tOkainSd6Q_cdwyPMpAyvs3ze2fCU20yhM1yp4VEHRc','Fabric Color!A:B').dropna()
    color_att = (nest_file['Sale Order Line/Product Attributes'].astype(str)+', ').str.extract(r'Color:\s(.+?),\s')
    color_att.columns = ['color']
    kwargs = {'Fabric Color': color_att['color']}
    nest_file = nest_file.assign(**kwargs) 
    nest_file = nest_file.merge(fabric_color,left_on='Fabric Color',right_on='CABA NAME',sort=False).drop('CABA NAME',axis=1)


    # s = []
    # for x in range(0,len(colorList)):
    #     s.append(re.sub(',.*','',test[x].split('Color: ')[1]))
    # d = {'Color': s}
    # color_att = pd.DataFrame(data=d)

    # add target week column
    df_week = nest_file['Sale Order Line/Commitment Date']
    df_week = pd.to_datetime(df_week, infer_datetime_format=True)  
    df_week = df_week.dt.tz_localize('UTC')
    a=df_week.dt.strftime('%W').fillna(0)
    a = np.array(a)
    a = a.astype(int)+1
    a = pd.DataFrame({'target week': a.tolist()})
    nest_file['So Target Week'] = a



    # rename covers to give chicory accent pillow size
    accentAttr = nest_file['Sale Order Line/Product Attributes'].fillna('blanks').tolist()
    for x in range(0,len(accentAttr)):
        accentAttr[x] =  re.sub(r'(?i)(?:.*\spillow\soptions\W\s|\sw/insert)','',accentAttr[x])
    accentAttr = pd.DataFrame({'attr':accentAttr}) 
    df4 = pd.DataFrame({'prod':nest_file['Covers']})
    df = nest_file['Covers'].str.cat(accentAttr['attr'],sep=' ')
    df1  = pd.DataFrame({'attrProduct':df})
    df5 = df4.where(~(df4['prod'].str.match('(.*for chicory.*)',case=False)),df1['attrProduct'],axis=0)
    nest_file['Covers'] =  df5
    nest_file = addInvValues(nest_file)
    nest_file['Fabric #'] = nest_file['Fabric #'].fillna(0)
    nest_file['Fabric #'] = nest_file['Fabric #'].astype(str).astype(int)
    return nest_file

def createSummaData(nest_file):
    # Mirror excel countif to generate sliced so line qty to produce
    tempList = nest_file['Sale Order Line/Product/Display Name'].fillna('blanks').tolist()
    bondi_chicory = []
    for x in range(0,len(tempList)):
        if (re.search('(?i)(?:mst1 bondi|mst1 chicory)',tempList[x])!=None):
            bondi_chicory.append((re.search('(?i)(?:mst1 bondi|mst1 chicory)',tempList[x])).group(0))
        else:
            bondi_chicory.append('other')
    nest_file['BONDI|CHICORY'] = bondi_chicory

    anaAeroAce = []
    for x in range(0,len(tempList)):
        if (re.search('(?i)(?:mst1 ana|mst1 ace|mst1 aero)',tempList[x])!=None):
            anaAeroAce.append((re.search('(?i)(?:mst1 ana|mst1 ace|mst1 aero)',tempList[x])).group(0))
        else:
            anaAeroAce.append('other')
    nest_file['ANA|AERO|ACE'] = anaAeroAce

    rename = nest_file['Sale Order Line/Product/Display Name'].fillna('blanks').tolist()
    rename2 = nest_file['Product/Display Name'].fillna('blanks').tolist()
    for x in range(0,len(rename)):
        rename[x] = re.sub(r'(?i)(.*\]\s)','',rename[x])
        rename2[x] = re.sub(r'(?i)(.*\]\s)','',rename2[x])
    to_assign = {'Product Name':rename,'Component Name':rename2}
    nest_file = nest_file.assign(**to_assign)

    # use numpy to create countif of so quantity
    so_count = []
    list1 = nest_file['Sale Order Line/Qty to Produce'].tolist()
    tempList = nest_file['Sale Order Line/ID'].tolist()
    for x in tempList:
       so_count.append(tempList.count(x))
    ar1 = np.array(list1)
    ar2 = np.array(so_count)
    ar3 = ar1 / ar2
    nest_file['SO Line Product Qty'] = ar3
    ar4 =  nest_file['Quantity To Be Produced'].tolist()
    #nest_file['Qty per SO qty'] = ar4 / ar1

    """
    Summa wo data detailed table

        """
    # creating column for fabric number

    fabric_no = (nest_file['First Raw Material/Display Name'].astype(str)+', ').str.extract(r'\]\s(\d{3}?)\s')
    fabric_no.columns = ['color']
    kwargs = {'Fabric #': fabric_no['color']}
    nest_file = nest_file.assign(**kwargs) 



    # add target week column
    df_week = nest_file['Sale Order Line/Commitment Date']
    df_week = pd.to_datetime(df_week, infer_datetime_format=True)  
    df_week = df_week.dt.tz_localize('UTC')
    a=df_week.dt.strftime('%W').fillna(0)
    a = np.array(a)
    a = a.astype(int)+1
    a = pd.DataFrame({'target week': a.tolist()})
    nest_file['So Target Week'] = a
    nest_file = addInvValues(nest_file)
    nest_file['Fabric #'] = nest_file['Fabric #'].fillna(0)
    nest_file['Fabric #'] = nest_file['Fabric #'].astype(str).astype(int)

    return nest_file

def createSewingPivot(dict,nest_file):


    gsheetId = dict['gsheetId']
    sheetProperties = dict['sheet_names']
    Url = dict['Url']
    sheet_names = [sheetProperties[0]['properties']['title'],sheetProperties[1]['properties']['title']]
    sheetIds = [sheetProperties[0]['properties']['sheetId'],sheetProperties[1]['properties']['sheetId']]


    # rename worksheet to Sewing
    request_body = {
      'requests': [
         {
           'updateSpreadsheetProperties': {
                'properties':  {
                     'title': 'Sewing Nesting API'
                },
              'fields': 'title'   
           }

        } 
      ]

    }
    request = service.spreadsheets().batchUpdate(
        spreadsheetId=gsheetId,
        body=request_body
    ).execute()



    # In[ ]:


    # PivotTable JSON Template
    request_body = {
        'requests': [
            {
                'updateCells': {
                    'rows': {
                        'values': [
                            {
                                'pivotTable': {
                                    # Data Source
                                    'source': {
                                        'sheetId': sheetIds[0],
                                        'startRowIndex': 0,
                                        'startColumnIndex': 0,
                                        'endRowIndex': len(nest_file),
                                        'endColumnIndex': len(nest_file.columns) # base index is 1
                                    },

                                    # Rows Field(s)
                                    'rows': [
                                        # row field #1
                                        {
                                            'sourceColumnOffset': nest_file.columns.get_loc('Fabric #'),
                                            'showTotals': True, # display subtotals
                                            'sortOrder': 'ASCENDING',
                                            'repeatHeadings': False,
                                            'label': 'Fabric #',
                                         },

                                        {
                                            'sourceColumnOffset': nest_file.columns.get_loc('Lot/Serial Number/Lot/Serial Number'),
                                            'showTotals': True, # display subtotals
                                            'sortOrder': 'ASCENDING',
                                            'repeatHeadings': False,
    #                                         'label': 'Product List',
                                        },

                                        {
                                            'sourceColumnOffset': nest_file.columns.get_loc('Sale Order Line/Product/Display Name'),
                                            'showTotals': False, # display subtotals
                                            'sortOrder': 'ASCENDING',
                                            'repeatHeadings': False,
    #                                         'label': 'Product List',
                                        },

                                        {
                                            'sourceColumnOffset': nest_file.columns.get_loc('Sale Order Line/Qty to Produce'),
                                            'showTotals': False, # display subtotals
                                            'sortOrder': 'ASCENDING',
                                            'repeatHeadings': False,
    #                                         'label': 'Product List',
                                        }

    #                                     {
    #                                         'sourceColumnOffset': 16,
    #                                         'showTotals': False, # display subtotals
    #                                         'sortOrder': 'ASCENDING',
    #                                         'repeatHeadings': False,
    # #                                         'label': 'Product List',
    #                                     }                   

                                    ],

    #                                 Columns Field(s)
                                    'columns': [
                                        # column field #1
                                        {
                                            'sourceColumnOffset': nest_file.columns.get_loc('Covers'),
                                            'sortOrder': 'ASCENDING', 
                                            'showTotals': True
                                        }
                                    ],

                                    'criteria': {
                                        nest_file.columns.get_loc('Operation/Display Name'): {
                                            'visibleValues': [
                                                'Sewing QC/Prep'
                                            ]
                                        },

                                        nest_file.columns.get_loc('BONDI|CHICORY'): {
                                            'visibleValues': [
                                                'MST1 Bondi', 'MST1 Chicory'
                                            ]
                                        },
                                           nest_file.columns.get_loc('Assigned to/Display Name'): {
                                            'visibleValues': [
                                                'FALSE'
                                            ]
                                        },

    #                                     11: {
    #                                         'condition': {
    #                                             'type': 'NUMBER_BETWEEN',
    #                                             'values': [
    #                                                 {
    #                                                     'userEnteredValue': '10000'
    #                                                 },
    #                                                 {
    #                                                     'userEnteredValue': '100000'
    #                                                 }
    #                                             ]
    #                                         },
    #                                         'visibleByDefault': True
    #                                     }
                                    },

                                    # Values Field(s)
                                    'values': [
                                        # value field #1
                                        {
                                            'sourceColumnOffset': nest_file.columns.get_loc('Quantity To Be Produced'),
                                            'summarizeFunction': 'SUM',
                                            'name': 'Covers Qty:'
                                        }
                                    ],

                                    'valueLayout': 'HORIZONTAL'
                                }
                            }
                        ]
                    },

                    'start': {
                        'sheetId': sheetIds[1],
                        'rowIndex': 0, # 4th row
                        'columnIndex': 0 # 3rd column
                    },
                    'fields': 'pivotTable'
                }
            }
        ]
    }

    response = service.spreadsheets().batchUpdate(
        spreadsheetId=gsheetId,
        body=request_body
    ).execute()


def createSummaPivot(dict,nest_file):


    gsheetId = dict['gsheetId']
    sheetProperties = dict['sheet_names']
    Url = dict['Url']
    sheet_names = [sheetProperties[0]['properties']['title'],sheetProperties[1]['properties']['title']]
    sheetIds = [sheetProperties[0]['properties']['sheetId'],sheetProperties[1]['properties']['sheetId']]

    # rename worksheet to Summa
    request_body = {
      'requests': [
         {
           'updateSpreadsheetProperties': {
                'properties':  {
                     'title': 'Summa Nesting API'
                },
              'fields': 'title'   
           }

        } 
      ]

    }
    request = service.spreadsheets().batchUpdate(
        spreadsheetId=gsheetId,
        body=request_body
    ).execute()
    


    """
    create pivot table using data inserted to sheets

    """

    # PivotTable JSON Template
    request_body = {
        'requests': [
            {
                'updateCells': {
                    'rows': {
                        'values': [
                            {
                                'pivotTable': {
                                    # Data Source
                                    'source': {
                                        'sheetId': sheetIds[0],
                                        'startRowIndex': 0,
                                        'startColumnIndex': 0,
                                        'endRowIndex': len(nest_file),
                                        'endColumnIndex': len(nest_file.columns) # base index is 1
                                    },

                                    # Rows Field(s)
                                    'rows': [
                                        # row field #1
                                        {
                                            'sourceColumnOffset': nest_file.columns.get_loc('Fabric #'),
                                            'showTotals': True, # display subtotals
                                            'sortOrder': 'ASCENDING',
                                            'repeatHeadings': False,
                                            'label': 'Fabric #',
                                         },

                                        {
                                            'sourceColumnOffset': nest_file.columns.get_loc('Sale Order Line/Product/Display Name'),
                                            'showTotals': True, # display subtotals
                                            'sortOrder': 'ASCENDING',
                                            'repeatHeadings': False,
    #                                         'label': 'Product List',
                                        },

                                        {
                                            'sourceColumnOffset': nest_file.columns.get_loc('Lot/Serial Number/Lot/Serial Number'),
                                            'showTotals': False, # display subtotals
                                            'sortOrder': 'ASCENDING',
                                            'repeatHeadings': False,
    #                                         'label': 'Product List',
                                        },

                                        {
                                            'sourceColumnOffset': nest_file.columns.get_loc('Sale Order Line/Product Attributes'),
                                            'showTotals': False, # display subtotals
                                            'sortOrder': 'ASCENDING',
                                            'repeatHeadings': False,
    #                                         'label': 'Product List',
                                        },
                                        
                                        {
                                            'sourceColumnOffset': nest_file.columns.get_loc('Sale Order Line/Qty to Produce'),
                                            'showTotals': False, # display subtotals
                                            'sortOrder': 'ASCENDING',
                                            'repeatHeadings': False,
    #                                         'label': 'Product List',
                                        }

    #                                     {
    #                                         'sourceColumnOffset': 16,
    #                                         'showTotals': False, # display subtotals
    #                                         'sortOrder': 'ASCENDING',
    #                                         'repeatHeadings': False,
    # #                                         'label': 'Product List',
    #                                     }                   

                                    ],

    #                                 Columns Field(s)
#                                     'columns': [
#                                         # column field #1
#                                         {
#                                             'sourceColumnOffset': nest_file.columns.get_loc('Covers'),
#                                             'sortOrder': 'ASCENDING', 
#                                             'showTotals': True
#                                         }
#                                     ],

                                    'criteria': {
#                                         nest_file.columns.get_loc('Operation/Display Name'): {
#                                             'visibleValues': [
#                                                 'Sewing QC/Prep'
#                                             ]
#                                         },

                                        nest_file.columns.get_loc('ANA|AERO|ACE'): {
                                            'visibleValues': [
                                                'MST1 ANA', 'MST1 Aero, MST1 Ace'
                                            ]
                                        },
                                           nest_file.columns.get_loc('Assigned to/Display Name'): {
                                            'visibleValues': [
                                                'FALSE'
                                            ]
                                        },

    #                                     11: {
    #                                         'condition': {
    #                                             'type': 'NUMBER_BETWEEN',
    #                                             'values': [
    #                                                 {
    #                                                     'userEnteredValue': '10000'
    #                                                 },
    #                                                 {
    #                                                     'userEnteredValue': '100000'
    #                                                 }
    #                                             ]
    #                                         },
    #                                         'visibleByDefault': True
    #                                     }
                                    },

                                    # Values Field(s)
                                    'values': [
                                        # value field #1
                                        {
                                            'sourceColumnOffset': nest_file.columns.get_loc('Quantity To Be Produced'),
                                            'summarizeFunction': 'SUM',
                                            'name': 'SO Line Product Qty'
                                        }
                                    ],

                                    'valueLayout': 'HORIZONTAL'
                                }
                            }
                        ]
                    },

                    'start': {
                        'sheetId': sheetIds[1],
                        'rowIndex': 0, # 4th row
                        'columnIndex': 0 # 3rd column
                    },
                    'fields': 'pivotTable'
                }
            }
        ]
    }

    response = service.spreadsheets().batchUpdate(
        spreadsheetId=gsheetId,
        body=request_body
    ).execute()
    
def createSummaPivotConsumption(dict,Url,nest_file):

    gsheetId = dict['gsheetId']
    add_sheets(gsheetId, 'Pivot Fabric Consumption')
    print('next is get sheet')
    sheetProperties = getSheetProperties(Url)
    print('get sheet is done')
    sheet_names = [sheetProperties[0]['properties']['title'],sheetProperties[2]['properties']['title']]
    sheetIds = [sheetProperties[0]['properties']['sheetId'],sheetProperties[2]['properties']['sheetId']]
    print(sheetIds)
    print(Url)


    """
    create pivot table using data inserted to sheets

    """

    # PivotTable JSON Template
    request_body = {
        'requests': [
            {
                'updateCells': {
                    'rows': {
                        'values': [
                            {
                                'pivotTable': {
                                    # Data Source
                                    'source': {
                                        'sheetId': sheetIds[0],
                                        'startRowIndex': 0,
                                        'startColumnIndex': 0,
                                        'endRowIndex': len(nest_file),
                                        'endColumnIndex': len(nest_file.columns) # base index is 1
                                    },

                                    # Rows Field(s)
                                    'rows': [
                                        # row field #1
                                        {
                                            'sourceColumnOffset': nest_file.columns.get_loc('Fabric #'),
                                            'showTotals': True, # display subtotals
                                            'sortOrder': 'ASCENDING',
                                            'repeatHeadings': False,
                                            'label': 'Fabric #',
                                         },

                                        {
                                            'sourceColumnOffset': nest_file.columns.get_loc('Sale Order Line/Product/Display Name'),
                                            'showTotals': True, # display subtotals
                                            'sortOrder': 'ASCENDING',
                                            'repeatHeadings': False,
    #                                         'label': 'Product List',
                                        },

                                        {
                                            'sourceColumnOffset': nest_file.columns.get_loc('Lot/Serial Number/Lot/Serial Number'),
                                            'showTotals': False, # display subtotals
                                            'sortOrder': 'ASCENDING',
                                            'repeatHeadings': False,
    #                                         'label': 'Product List',
                                        },

                                        {
                                            'sourceColumnOffset': nest_file.columns.get_loc('Sale Order Line/Product Attributes'),
                                            'showTotals': False, # display subtotals
                                            'sortOrder': 'ASCENDING',
                                            'repeatHeadings': False,
    #                                         'label': 'Product List',
                                        },
                                        
                                        {
                                            'sourceColumnOffset': nest_file.columns.get_loc('Sale Order Line/Qty to Produce'),
                                            'showTotals': False, # display subtotals
                                            'sortOrder': 'ASCENDING',
                                            'repeatHeadings': False,
    #                                         'label': 'Product List',
                                        }

    #                                     {
    #                                         'sourceColumnOffset': 16,
    #                                         'showTotals': False, # display subtotals
    #                                         'sortOrder': 'ASCENDING',
    #                                         'repeatHeadings': False,
    # #                                         'label': 'Product List',
    #                                     }                   

                                    ],

    #                                 Columns Field(s)
#                                     'columns': [
#                                         # column field #1
#                                         {
#                                             'sourceColumnOffset': nest_file.columns.get_loc('Covers'),
#                                             'sortOrder': 'ASCENDING', 
#                                             'showTotals': True
#                                         }
#                                     ],

                                    'criteria': {
#                                         nest_file.columns.get_loc('Operation/Display Name'): {
#                                             'visibleValues': [
#                                                 'Sewing QC/Prep'
#                                             ]
#                                         },

                                        nest_file.columns.get_loc('ANA|AERO|ACE'): {
                                            'visibleValues': [
                                                'MST1 ANA', 'MST1 Aero, MST1 Ace'
                                            ]
                                        },
                                           nest_file.columns.get_loc('Assigned to/Display Name'): {
                                            'visibleValues': [
                                                'FALSE'
                                            ]
                                        },

    #                                     11: {
    #                                         'condition': {
    #                                             'type': 'NUMBER_BETWEEN',
    #                                             'values': [
    #                                                 {
    #                                                     'userEnteredValue': '10000'
    #                                                 },
    #                                                 {
    #                                                     'userEnteredValue': '100000'
    #                                                 }
    #                                             ]
    #                                         },
    #                                         'visibleByDefault': True
    #                                     }
                                    },

                                    # Values Field(s)
                                    'values': [
                                        # value field #1
                                        {
                                            'sourceColumnOffset': nest_file.columns.get_loc('Quantity To Be Produced'),
                                            'summarizeFunction': 'SUM',
                                            'name': 'SO Line Product Qty'
                                        }
                                    ],

                                    'valueLayout': 'HORIZONTAL'
                                }
                            }
                        ]
                    },

                    'start': {
                        'sheetId': sheetIds[1],
                        'rowIndex': 0, # 4th row
                        'columnIndex': 0 # 3rd column
                    },
                    'fields': 'pivotTable'
                }
            }
        ]
    }

    response = service.spreadsheets().batchUpdate(
        spreadsheetId=gsheetId,
        body=request_body
    ).execute()
    













#!/usr/bin/env python
# coding: utf-8

# In[1]:


# required libararies
import pandas as pd;
from googleapiclient.discovery import build;
from google_auth_oauthlib.flow import InstalledAppFlow,Flow;
from google.auth.transport.requests import Request;
import os;
import pickle;


# In[4]:


"""
Authenticated Google API by a downloaded JSON file. 
by using this file, we created â€˜token.pickleâ€™ 
file which will be stored in our pc 
for future use and whenever this pickle 
file will expire our code will refresh the file.
"""
FILE_NAME = 'client_secret_file.json'
SCOPES = ['https://www.googleapis.com/auth/spreadsheets']

def main(gsheetId,SAMPLE_RANGE_NAME):
    global values_input, service
    creds = None
    if os.path.exists('token.pickle'):
        with open('token.pickle', 'rb') as token:
            creds = pickle.load(token)
    if not creds or not creds.valid:
        if creds and creds.expired and creds.refresh_token:
            creds.refresh(Request())
        else:
            flow = InstalledAppFlow.from_client_secrets_file(
                FILE_NAME, SCOPES) # here enter the name of your downloaded JSON file
            creds = flow.run_local_server(port=0)
        with open('token.pickle', 'wb') as token:
            pickle.dump(creds, token)

    service = build('sheets', 'v4', credentials=creds)
    
     # Call the Sheets API
    sheet = service.spreadsheets()
    result_input = sheet.values().get(spreadsheetId=gsheetId,
                                range=SAMPLE_RANGE_NAME).execute()
    values_input = result_input.get('values', [])

    if not values_input and not values_expansion:
        print('No data found.')
    return pd.DataFrame(values_input[1:], columns=values_input[0])



#!/usr/bin/env python
# coding: utf-8

# In[ ]:


from selenium import webdriver
from bs4 import BeautifulSoup
import urllib.request
import time
import pandas as pd
import re


# In[ ]:


def scrape_tracking(url):
    driver=webdriver.Chrome()
    driver.get(url)
    button=driver.find_element('xpath', '//*[@id="btnSubmit"]')
    button.click()
    time.sleep(2)
    html_source=driver.page_source
    dfs=pd.read_html(html_source)
    result=' '.join(map(str,dfs))
    result=re.sub(r"(?:\n|\\n\d{2}|\\n\d{1}|NaN)",'',result)
    result=' '.join(map(str,result.split()))
    shipment_created=result.find('Shipment created.')>0
    shipped=result.find('Shipment Status changed')>0
    return {'shipment_created':shipment_created, 'shipped':shipped}

